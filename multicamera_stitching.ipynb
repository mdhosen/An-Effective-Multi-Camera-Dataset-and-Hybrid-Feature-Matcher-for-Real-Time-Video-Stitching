{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aa7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import tqdm\n",
    "import os\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from random import randint\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from scipy.spatial import cKDTree\n",
    "#import tensorflow as tf\n",
    "#print(tf.test.gpu_device_name())\n",
    "\n",
    "class VideoStitcher:\n",
    "    def __init__(self, left_video_in_path,\n",
    "                 right_video_in_path,\n",
    "                 left_video_in_path2,\n",
    "                 right_video_in_path2,\n",
    "                 video_out_path, video_out_width=800,\n",
    "                 display=True):\n",
    "        # Initialize arguments\n",
    "        self.left_video_in_path = left_video_in_path\n",
    "        self.right_video_in_path = right_video_in_path\n",
    "        self.left_video_in_path2 = left_video_in_path2\n",
    "        self.right_video_in_path2 = right_video_in_path2\n",
    "        self.video_out_path = video_out_path\n",
    "        self.video_out_width = video_out_width\n",
    "        self.display = display\n",
    "\n",
    "        # Initialize the saved homography matrix\n",
    "        self.saved_homo_matrix1 = None\n",
    "        self.saved_homo_matrix2 = None\n",
    "        self.saved_homo_matrix3 = None\n",
    "\n",
    "    def stitch(self, images, ratio=0.75, reproj_thresh=4.0):\n",
    "        # Unpack the images\n",
    "        (image_d, image_c, image_b, image_a) = images\n",
    "        \n",
    "        image_dd = cv2.cvtColor(image_d, cv2.COLOR_RGB2GRAY)\n",
    "        image_cc = cv2.cvtColor(image_c, cv2.COLOR_RGB2GRAY)\n",
    "        image_bb = cv2.cvtColor(image_b, cv2.COLOR_RGB2GRAY)\n",
    "        image_aa = cv2.cvtColor(image_a, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # If the saved homography matrix is None, then we need to apply keypoint matching to construct it\n",
    "        if self.saved_homo_matrix1 is None:\n",
    "            # Detect keypoints and extract\n",
    "            tfirst= time.time()\n",
    "            (keypoints_a, features_a) = self.detect_and_extract(image_aa)\n",
    "            print(time.time()-tfirst)\n",
    "            (keypoints_b, features_b) = self.detect_and_extract(image_bb)\n",
    "            tfirst1= time.time()\n",
    "            matched_keypoints1 = self.match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh)\n",
    "            print(time.time()-tfirst1)\n",
    "            if matched_keypoints1  is None:\n",
    "                return None\n",
    "            self.saved_homo_matrix1 = matched_keypoints1[1]\n",
    "            print('h1')\n",
    "            \n",
    "            \n",
    "        hmgphy1 = self.saved_homo_matrix1\n",
    "        \n",
    "        if self.saved_homo_matrix2 is None:\n",
    "            (keypoints_c, features_c) = self.detect_and_extract(image_cc)\n",
    "            matched_keypoints2 = self.match_keypoints(keypoints_b, keypoints_c, features_b, features_c, ratio, reproj_thresh)\n",
    "\n",
    "            if matched_keypoints2  is None:\n",
    "                return None\n",
    "            self.saved_homo_matrix2 = matched_keypoints2[1]\n",
    "            print('h2')\n",
    "            \n",
    "        hmgphy2 = self.saved_homo_matrix2\n",
    "        \n",
    "        if self.saved_homo_matrix3 is None:\n",
    "            (keypoints_d, features_d) = self.detect_and_extract(image_dd)\n",
    "            matched_keypoints3 = self.match_keypoints(keypoints_c, keypoints_d, features_c, features_d, ratio, reproj_thresh)\n",
    "\n",
    "            if matched_keypoints3  is None:\n",
    "                return None\n",
    "            self.saved_homo_matrix3 = matched_keypoints3[1]\n",
    "            print('h3')\n",
    "            \n",
    "        hmgphy3 = self.saved_homo_matrix3\n",
    "            \n",
    "        output_shape1 = (image_a.shape[1] + image_b.shape[1], image_a.shape[0])\n",
    "        resultfab = cv2.warpPerspective(image_a, hmgphy1, output_shape1)\n",
    "        resultfab[0:image_b.shape[0], 0:image_b.shape[1]] = image_b\n",
    "        \n",
    "        #cv2.imwrite(\"separate/12.jpg\", cv2.cvtColor(resultfab, cv2.COLOR_BGR2RGB ))\n",
    "        \n",
    "        output_shape2 = (resultfab.shape[1] + image_c.shape[1], resultfab.shape[0])\n",
    "        resultfabc = cv2.warpPerspective(resultfab, hmgphy2, output_shape2)\n",
    "        resultfabc[0:image_c.shape[0], 0:image_c.shape[1]] = image_c\n",
    "        \n",
    "        #cv2.imwrite(\"separate/123.jpg\", cv2.cvtColor(resultfabc, cv2.COLOR_BGR2RGB ))\n",
    "        \n",
    "        output_shape3 = (resultfabc.shape[1] + image_d.shape[1], resultfabc.shape[0])\n",
    "        resultfabcd = cv2.warpPerspective(resultfabc, hmgphy3, output_shape3)\n",
    "        resultfabcd[0:image_d.shape[0], 0:image_d.shape[1]] = image_d\n",
    "        \n",
    "        #cv2.imwrite(\"separate/1234.jpg\", cv2.cvtColor(resultfabcd, cv2.COLOR_BGR2RGB ))\n",
    "        #resultfabcd=(resultfabcd/255.0)\n",
    "        return resultfabcd\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_and_extract(image):\n",
    "        # Detect and extract features from the image (DoG keypoint detector and SIFT feature extractor)\n",
    "        #descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        \n",
    "        #addd start\n",
    "        #detector = cv2.FeatureDetector_create(\"SIFT\")## original\n",
    "        #detector = cv2.ORB_create(nfeatures=8000, scaleFactor=1.1, nlevels=8, edgeThreshold=10, firstLevel=0, WTA_K=2, patchSize=10)\n",
    "\n",
    "        algo = cv2.xfeatures2d.SIFT_create( nOctaveLayers=3, contrastThreshold=0.04, edgeThreshold=10, sigma=1.6)\n",
    "        #detector = cv2.SIFT_create()\n",
    "        #detector = cv2.xfeatures2d.SIFT_create()\n",
    "        #detector = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "        #descriptor = cv2.DescriptorExtractor_create(\"SIFT\")###original\n",
    "        #descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        \n",
    "        #keypoints = algo.detect(image)\n",
    "        #keypoints, features = algo.compute(image, keypoints)\n",
    "        \n",
    "        eps = 1e-7\n",
    "        keypoints = algo.detect(image)\n",
    "        keypoints, features = algo.compute(image, keypoints)\n",
    "        features /= (features.sum(axis=1, keepdims=True) + eps)\n",
    "        features = np.sqrt(features)\n",
    "\n",
    "\n",
    "\n",
    "        #(keypoints, features) = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "        # Convert the keypoints from KeyPoint objects to numpy arrays\n",
    "        keypoints = np.float32([keypoint.pt for keypoint in keypoints])\n",
    "\n",
    "        # Return a tuple of keypoints and features\n",
    "        return (keypoints, features)\n",
    "\n",
    "    @staticmethod\n",
    "    def match_keypoints(keypoints_a, keypoints_b, features_a, features_b, ratio, reproj_thresh):\n",
    "        # Compute the raw matches and initialize the list of actual matches\n",
    "        #matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        #matcher = cv.BFMatcher()\n",
    "        ################\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)   # or pass empty dictionary\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        flann_matches = flann.knnMatch(features_a, features_b, k=2)\n",
    "        print(\" flann_matches raw matches with rootSift: \",len(flann_matches))\n",
    "        #print(flann_matches)\n",
    "        #########################\n",
    "        bf = cv2.BFMatcher()\n",
    "        bf_matches = bf.knnMatch(features_a,features_b,k=2)\n",
    "        print(\" BF raw matches with rootSift: \",len(bf_matches))\n",
    "        #print(type(bf_matches))\n",
    "        #print(bf_matches)\n",
    "        raw_matches = np.concatenate([flann_matches, bf_matches],axis=0)\n",
    "        #in_first = set(flann_matches)\n",
    "        #in_second = set(bf_matches)\n",
    "        #in_second_but_not_in_first = in_second - in_first\n",
    "        #result = first_list + list(in_second_but_not_in_first)\n",
    "        #pA=[[0,0,0],[0,1,0],[1,2,4],[10,3,4],[1,20,1],[5,3,2]]\n",
    "        #pB=[[14,1,0],[1,2,4],[1,20,1],[15,1,0]]\n",
    "        #kdtree = cKDTree(flann_matches)\n",
    "      \n",
    "        matches = []\n",
    "\n",
    "        for raw_match in raw_matches:\n",
    "            # Ensure the distance is within a certain ratio of each other (i.e. Lowe's ratio test)\n",
    "            if len(raw_match) == 2 and raw_match[0].distance < raw_match[1].distance * ratio:\n",
    "                matches.append((raw_match[0].trainIdx, raw_match[0].queryIdx))\n",
    "        \n",
    "        print(\" combine good matches with rootSift: \",len(matches))\n",
    "        \n",
    "        matches = list(set(matches))\n",
    "        \n",
    "        print(\" combine good matches with rootSift unique value : \", len(matches))\n",
    "\n",
    "        # Computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # Construct the two sets of points\n",
    "            points_a = np.float32([keypoints_a[i] for (_, i) in matches])\n",
    "            points_b = np.float32([keypoints_b[i] for (i, _) in matches])\n",
    "\n",
    "            # Compute the homography between the two sets of points\n",
    "            (homography_matrix, status) = cv2.findHomography(points_a, points_b, cv2.RANSAC, reproj_thresh)\n",
    "\n",
    "            # Return the matches, homography matrix and status of each matched point\n",
    "            return (matches, homography_matrix, status)\n",
    "\n",
    "        # No homography could be computed\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_matches(image_a, image_b, keypoints_a, keypoints_b, matches, status):\n",
    "        # Initialize the output visualization image\n",
    "        (height_a, width_a) = image_a.shape[:2]\n",
    "        (height_b, width_b) = image_b.shape[:2]\n",
    "        visualisation = np.zeros((max(height_a, height_b), width_a + width_b, 3), dtype=\"uint8\")\n",
    "        visualisation[0:height_a, 0:width_a] = image_a\n",
    "        visualisation[0:height_b, width_a:] = image_b\n",
    "\n",
    "        for ((train_index, query_index), s) in zip(matches, status):\n",
    "            # Only process the match if the keypoint was successfully matched\n",
    "            if s == 1:\n",
    "                # Draw the match\n",
    "                point_a = (int(keypoints_a[query_index][0]), int(keypoints_a[query_index][1]))\n",
    "                point_b = (int(keypoints_b[train_index][0]) + width_a, int(keypoints_b[train_index][1]))\n",
    "                cv2.line(visualisation, point_a, point_b, (0, 255, 0), 1)\n",
    "\n",
    "        # return the visualization\n",
    "        \n",
    "        \n",
    "        return visualisation\n",
    "\n",
    "    def run(self):\n",
    "        # Set up video capture\n",
    "        left_video = cv2.VideoCapture(self.left_video_in_path)\n",
    "        if left_video is None:\n",
    "            print(\" no left_video \")\n",
    "           \n",
    "        right_video = cv2.VideoCapture(self.right_video_in_path)\n",
    "        if right_video is None:\n",
    "            print(\" no right_video \")\n",
    "          \n",
    "        left_video2 = cv2.VideoCapture(self.left_video_in_path2)\n",
    "        if left_video2 is None:\n",
    "            print(\" no left_video2 \")\n",
    "           \n",
    "        \n",
    "        right_video2 = cv2.VideoCapture(self.right_video_in_path2)\n",
    "        \n",
    "        if right_video2 is None:\n",
    "            print(\" no right_video2\")\n",
    "            \n",
    "            \n",
    "        print('[INFO]: {} and {} and {} and {} loaded'.format(self.left_video_in_path.split('/')[-1],\n",
    "                                                              self.right_video_in_path.split('/')[-1],\n",
    "                                                              self.left_video_in_path2.split('/')[-1],\n",
    "                                                              self.right_video_in_path2.split('/')[-1]))\n",
    "        print('[INFO]: Video stitching starting....')\n",
    "\n",
    "        # Get information about the videos\n",
    "        n_frames = min(int(left_video.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(right_video.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(left_video2.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
    "                       int(right_video2.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        \n",
    "        #fps = int(left_video.get(cv2.CAP_PROP_FPS))\n",
    "        fps=15\n",
    "        frames = []\n",
    "\n",
    "        for _ in tqdm.tqdm(np.arange(n_frames)):\n",
    "            # Grab the frames from their respective video streams\n",
    "            ok, img1 = left_video.read()\n",
    "            _, img2 = right_video.read()\n",
    "            ok2, img3 = left_video2.read()\n",
    "            _, img4 = right_video2.read()\n",
    "            \n",
    "            \n",
    "            if ok & ok2 :\n",
    "                # Stitch the frames together to form the panorama\n",
    "                #left = left.astype('uint8')\n",
    "\n",
    "              \n",
    "                img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "                #cv2.imwrite(\"separate/f1.jpg\", cv2.cvtColor(img1, cv2.COLOR_BGR2RGB ))\n",
    "                \n",
    "                img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "                #cv2.imwrite(\"separate/f2.jpg\", cv2.cvtColor(img2, cv2.COLOR_BGR2RGB ))\n",
    "                \n",
    "                img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "                #cv2.imwrite(\"separate/f3.jpg\", cv2.cvtColor(img3, cv2.COLOR_BGR2RGB ))\n",
    "                \n",
    "                img4 = cv2.cvtColor(img4, cv2.COLOR_BGR2RGB)\n",
    "                #cv2.imwrite(\"separate/f4.jpg\", cv2.cvtColor(img4, cv2.COLOR_BGR2RGB ))\n",
    "                \n",
    "\n",
    "                stitched_frame = self.stitch([img1, img2, img3, img4])\n",
    "                \n",
    "        \n",
    "                # No homography could not be computed\n",
    "                if stitched_frame is None:\n",
    "                    print(\"[INFO]: Homography could not be computed!\")\n",
    "                    break\n",
    "\n",
    "                # Add frame to video\n",
    "                #stitched_frame1 = stitched_frame1.astype('uint8')\n",
    "                #stitched_frame2 = stitched_frame2.astype('uint8')\n",
    "                \n",
    "                \n",
    "                #print(stitched_frame.shape)\n",
    "                stitched_frame = imutils.resize(stitched_frame, width=self.video_out_width)                \n",
    "              \n",
    "                frames.append(stitched_frame)\n",
    "                \n",
    "                cnvc=cv2.cvtColor(stitched_frame,cv2.cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                \n",
    "\n",
    "                if self.display:\n",
    "                    #Show the output images\n",
    "                    cv2.imshow('output', cnvc)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                        break\n",
    "           \n",
    "        cv2.destroyAllWindows()\n",
    "        print('[INFO]: Video stitching finished')\n",
    "\n",
    "        # Save video\n",
    "        print('[INFO]: Saving {} in {}'.format(self.video_out_path.split('/')[-1],\n",
    "                                               os.path.dirname(self.video_out_path)))\n",
    "        #print(\"fps: \", fps)\n",
    "        clip = ImageSequenceClip(frames, fps=fps)\n",
    " \n",
    "        clip.write_videofile(self.video_out_path, codec ='mpeg4', audio=False,\n",
    "                             verbose=False, bitrate=\"50000k\")\n",
    "        print('[INFO]: {} saved'.format(self.video_out_path.split('/')[-1]))\n",
    "\n",
    "\n",
    "# Example call to 'VideoStitcher'\n",
    "#for i in range(1,101):\n",
    "    \n",
    "    \n",
    "# Example call to 'VideoStitcher'\n",
    "rootDir =\"videos/\"     \n",
    "# Example call to 'VideoStitcher'q\n",
    "stitcher = VideoStitcher(left_video_in_path= rootDir + \"test1.avi\",\n",
    "                         right_video_in_path=rootDir + \"test2.avi\",\n",
    "                         left_video_in_path2=rootDir + \"test3.avi\",\n",
    "                         right_video_in_path2=rootDir + \"test4.avi\",\n",
    "                         video_out_path=rootDir + \"testResult.avi\")\n",
    "                         #\n",
    "\n",
    "stitcher.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e25d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214a4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
